---
title: "C7 Week 1"
author: "JAGV"
date: "2024-05-28"
output: html_document
editor_options: 
  chunk_output_type: inline
---

# Regression

Regression models are the workhorse of data science. They are the most well described, practical and theoretically understood models in statistics. A Data Scientist well versed in Regression Models will be able to solve an incredible array of problems. 

One key insight for regression models is that they produce highly interpretable model fits. This is unlike the machine learning algorithms,which often sacrifice interpretability for improved prediction performance or automation. These are, of course, valuable attributes in their own rights. However, the benefit of simplicity, parsimony and interpretability offered by regression models (and their close generalizations) should make them a first tool of choice for any practical problem. 

## Questions for this class

- To use the parents' heights to predict childrens' heights
- To try to find a parsimonious, easily described mean relationship between parent and childrens' heights. 
- To investigate the variation in childrens' heights that appears unrelated to parents' heights (residual variation).
- To quantify what impact genotype information has beyond parental height in explaining child height. 
- To figure out how/whether and what assumptions are needed to generalize findings beyond the data in question. 
- Why do children of very tall parents tend to be tall, but a little shorter than their parents and why children of very short parents tend to be short, but a little bit taller than their parents? (This is a famous question called *Regression to the mean*).

It was [Francis Galton](https://es.wikipedia.org/wiki/Francis_Galton) the one who invented the term and concepts of regression and correlation. His paper is entitled *Regression towars mediocrity in hereditary stature* (published in the Journal of the Anthropological Institute of Great Britain and Ireland, Vol. 15 (1886)). *Fun fact*: he was the cousin of Charles Darwin. 

The Galton data is found in the R package `UsingR`

```{r}
library(UsingR)
data(galton)
library(reshape)
long <- melt(galton)
library(ggplot2)
g <- ggplot(long, aes(x = value, fill = variable))
g <- g + geom_histogram(colour = "black", binwidth = 1)
g <- g + facet_grid(.~ variable)
g
```

Observations: 

We can define "the middle" of the distribution as the value of $\mu$ that minimizes 

\[
    \sum_{i=1}^n{\left(Y_i-\mu\right)^2}
\]
which is $\mu = \bar{Y}$ and is considered the physical center of mass of the histogram. 


```{r}
library(manipulate)
myHist <- function(mu){
    mse <- mean((galton$child - mu)^2)
    g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth = 1)
    g <- g + geom_vline(xintercept = mu,size = 3)
    g <- g + ggtitle(paste("mu = ", mu, ", MSE = ", round(mse,2), sep = ""))
    g
}
#manipulate(myHist(mu), mu = slider(62,74, step = 0.5))

## Remember that there is a problem. Try to first run this code
#  manipulate(plot(rnorm(x)), x = slider(1, 100, step = 1))
## before running the previous one in order to show the gear to manipulate the slider.
```



Comparing childrens' heights and their parents' heights

```{r}
ggplot(galton, aes(x = parent, y = child))+geom_point()
```

Because there are multiple data points for the same parent/child combination a third dimension (size of point) should be used when constructing the scatterplot

```{r}
library(dplyr)
# Constructs table for different combination of parent-child height
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child (in)", "parent (in)", "freq")
# Convert to numerical values
freqData$child <-as.numeric(as.character(freqData$child))
freqData$parent <-as.numeric(as.character(freqData$parent))
# Filter to only meaningful combinations
g <- ggplot(filter(freqData, freq > 0), aes(x=parent, y = child))
g <- g + scale_size(range = c(2,20), guide = "none")
# Plot gray circles slightly larger than data as base (achieve an outline effect)
g <- g + geom_point(colour = "grey50", aes(size = freq+10, show_guide = FALSE))
# Plot the accurate data points
g <- g + geom_point(aes(colour = freq, size = freq))
# change the color gradient from default to lightblue -> $white
g <- g + scale_colour_gradient(low = "lightblue", high = "white")
g
```


Regression through origin can be achieved by substracting the mean of both variables $x$ and $Y$. This 

```{r}
library(manipulate)
x <- galton$parent - mean(galton$parent)
y <- galton$child - mean(galton$child)

freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
# Convert to numerical values
freqData$child <-as.numeric(as.character(freqData$child))
freqData$parent <-as.numeric(as.character(freqData$parent))

mySP <- function(beta){
    mse <- mean((y - beta*x)^2)
    g <- ggplot(filter(freqData, freq > 0), aes(x=parent, y = child))
    g <- g + scale_size(range = c(2,20), guide = "none")
    # Plot gray circles slightly larger than data as base (achieve an outline effect)
    g <- g + geom_point(colour = "grey50", aes(size = freq+20, show_guide = FALSE))
    # Plot the accurate data points
    g <- g + geom_point(aes(colour = freq, size = freq))
    # change the color gradient from default to lightblue -> $white
    g <- g + scale_colour_gradient(low = "lightblue", high = "white")
    g <- g + geom_abline(slope = beta, intercept = 0, size = 2)
    g <- g + ggtitle(paste("beta = ", beta, ", MSE = ", round(mse,2), sep = ""))
    g
}
#manipulate(mySP(beta), beta = slider(-2,2, step = 0.02))

## Remember that there is a problem. Try to first run this code
#  manipulate(plot(rnorm(x)), x = slider(1, 100, step = 1))
## before running the previous one in order to show the gear to manipulate the slider.
```



```{r}
lm(child ~ parent, data = galton)
```

# Linear least squares

## Some basic notation and background

When we have pairs of data $(X_i,Y_i)$ we define the (sample/empirical) covariance and the correlation as follows

\[
    Cov\left(X,Y\right) = \dfrac{1}{n-1}\sum_{i=1}^{n}\left(X_i  \bar{X}\right)\left(Y_i - \bar{Y}\right) = \dfrac{1}{n-1}\sum_{i=1}^{n}\left(X_iY_i - n\bar{X}\bar{Y}\right)
\]

\[
    Cor\left(X,Y\right) = \dfrac{Cov\left(X,Y\right)}{S_xS_y}
\]
where $S_x$ and $S_y$ are the sample standard deviation of $X$ and $Y$, respectively.

### Properties of correlation

- $Cor(X,Y) = Cor(Y,X)$
- $-1 \leq Cor(X,Y) \leq 1$
- $Cor(X,Y) = \pm 1$ only when the $X$ and $Y$ observations fall perfectly on a positive or negative sloped line, respectively.
- $Cor(X,Y)$ measures the strength of linear relationship between the $X$ and $Y$ data, with stronger relationships as $Cor(X,Y)$ heads towards $\pm 1$.
- $Cor(X,Y)= 0$ implies no **linear** relationship

## Linear Least Squares

The estimators for $\beta_0$ and $\beta_1$ can be derived following several approaches. One of those is the one called **least squares method** for which we define the **residual** as the difference between the observed value and the predicted value. That is to say

\[
    e_i = Y_{i} - \left(\underbrace{\beta_0 + \beta_1x_i}_{\text{The predicted $Y_i$ value}}\right)
\]
Then, the linear least squares method involves to calculate the values of $\beta_0$ and $\beta_1$ that minimizes the sum of squares of the residuals, which is a two variable function:
\[
    Q\left(\beta_0,\beta_1\right) = \sum_{i=1}^n{e_i^2} = \sum_{i=1}^{n}{\left(Y_i - \beta_0 - \beta_1x_i\right)^2}
\]
and from which, using Calculus, we can prove that 
\[
\hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{x} \hspace{5ex} \hat{\beta_1} = \dfrac{S_{xY}}{s_x^2} = r_{xy}\dfrac{S_y}{S_x}
\]
we can see that the units of $\hat{\beta_1}$ are the units of $\dfrac{Y}{X}$ whereas the units of $\hat{\beta_0}$ are the units of $Y$.

# Regression to the mean

```{r}
library(UsingR)
data(father.son)
y <- (father.son$sheight - mean(father.son$sheight))/sd(father.son$sheight)
x <- (father.son$fheight - mean(father.son$fheight))/sd(father.son$fheight)
rho <- cor(x,y)
library(ggplot2)
g <- ggplot(data.frame(x = x, y = y), aes(x = x, y = y))
g <- g + geom_point(size = 4, colour = "black", alpha = 0.2)
g <- g + geom_point(size = 2, colour = "salmon", alpha = 0.2)
g <- g + xlim(-4,4) + ylim(-4,4)
g <- g + geom_abline(intercept = 0, slope = 1)
g <- g + geom_vline(xintercept = 0)
g <- g + geom_hline(yintercept = 0)
g <- g + geom_abline(intercept = 0, slope = rho, size = 2)
g <- g + geom_abline(intercept = 0, slope = 1/rho, size = 2)
g
```

Comments: note that the correlation $\rho$ measures the extent of the regression to the mean, as every prediction would be obtained by multiplying $\rho\cdot x$.


# Quiz

```{r}
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
sum(w*x)/sum(w)
```
```{r}
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
plot(x,y)
fit <- lm(y ~ x)
fit$coefficients
```

```{r}
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
fit$coefficients
```

```{r}
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
(x-mean(x))/sd(x)
```

```{r}
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)


```


```{r}
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
```
```{r}
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)$coeff[2]/lm(x ~y)$coeff[2]
var(y)/var(x)
cor(y,x)
2*sd(y)/sd(x)

```

